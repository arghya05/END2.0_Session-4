{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_LSTM_Practice.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "edhClIWVO0km"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jofyc9OC4Qcf"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahBVnrNc3E0U"
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "plt.style.use('seaborn-white')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crQSAaIz4SkA"
      },
      "source": [
        "# Read and process data. \n",
        "\n",
        "Download the file from this URL: https://drive.google.com/file/d/1UWWIi-sz9g0x3LFvkIZjvK1r2ZaCqgGS/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WYQWjTLak6V",
        "outputId": "7eb1e8bc-8d42-4cec-85e4-1ebcbb467a8d"
      },
      "source": [
        "! gdown https://drive.google.com/uc?id=1UWWIi-sz9g0x3LFvkIZjvK1r2ZaCqgGS"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UWWIi-sz9g0x3LFvkIZjvK1r2ZaCqgGS\n",
            "To: /content/text.txt\n",
            "\r  0% 0.00/10.3k [00:00<?, ?B/s]\r100% 10.3k/10.3k [00:00<00:00, 17.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgOGxPDP3Wpp"
      },
      "source": [
        "data = open('text.txt', 'r').read()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeXXMLRb4kXb"
      },
      "source": [
        "Process data and calculate indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5TKeiOp4jtl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "630e78e6-e2d3-4825-c367-045242c4aa16"
      },
      "source": [
        "chars = list(set(data))\n",
        "data_size, X_size = len(data), len(chars)\n",
        "print(\"Corona Virus article has %d characters, %d unique characters\" %(data_size, X_size))\n",
        "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
        "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corona Virus article has 10223 characters, 75 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C53MB135LRY"
      },
      "source": [
        "# Constants and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfj21ORa49Ps"
      },
      "source": [
        "Hidden_Layer_size = 10 #size of the hidden layer\n",
        "Time_steps = 10 # Number of time steps (length of the sequence) used for training\n",
        "learning_rate = 1e-1 # Learning Rate\n",
        "weight_sd = 0.1 #Standard deviation of weights for initialization\n",
        "z_size = Hidden_Layer_size + X_size #Size of concatenation(H, X) vector"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdmJf4Du5uhb"
      },
      "source": [
        "# Activation Functions and Derivatives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seGHei_D5FGk"
      },
      "source": [
        "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def dsigmoid(y): return y * (1 - y)\n",
        "\n",
        "def tanh(x): return np.tanh(x)\n",
        "\n",
        "def dtanh(y): return 1 - y * y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE5sBCokMQPk"
      },
      "source": [
        "# Quiz Question 1\n",
        "\n",
        "What is the value of sigmoid(0) calculated from  your code? (Answer up to 1 decimal point, e.g. 4.2 and NOT 4.29999999, no rounding off)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DECYTOdoMPeW",
        "outputId": "e3d360f2-4653-4ce6-b0dc-cd5f92faea8f"
      },
      "source": [
        "sigmoid(0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldaF0o7mMbS1"
      },
      "source": [
        "# Quiz Question 2\n",
        "\n",
        "What is the value of dsigmoid(sigmoid(0)) calculated from your code?? (Answer up to 2 decimal point, e.g. 4.29 and NOT 4.29999999, no rounding off). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1i37ID6MdDo",
        "outputId": "b3d5d82e-2829-4190-f19b-c8fc8cf15e09"
      },
      "source": [
        "dsigmoid(sigmoid(0))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78D9IUC5MhTn"
      },
      "source": [
        "# Quiz Question 3\n",
        "\n",
        "What is the value of tanh(dsigmoid(sigmoid(0))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwwrPTopMi7M",
        "outputId": "e045ae6d-a346-4d92-ec2a-f0af19f8a358"
      },
      "source": [
        "tanh(dsigmoid(sigmoid(0)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24491866240370913"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeCvVH1v6Me-"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Quiz Question 4\n",
        "\n",
        "What is the value of dtanh(tanh(dsigmoid(sigmoid(0)))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8eRxHzPMoBr",
        "outputId": "d0f9451a-141b-4fe3-edb0-3f101db5d97d"
      },
      "source": [
        "dtanh(tanh(dsigmoid(sigmoid(0))))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.940014848806378"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeSVipDu8iKE"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICbWNemE6LGV"
      },
      "source": [
        "class Param:\n",
        "    def __init__(self, name, value):\n",
        "      self.name = name\n",
        "      self.v = value # parameter value\n",
        "      self.d = np.zeros_like(value) # derivative\n",
        "      self.m = np.zeros_like(value) # momentum for Adagrad"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j83pZNPE8212"
      },
      "source": [
        "We use random weights with normal distribution (0, weight_sd) for  tanh  activation function and (0.5, weight_sd) for  `sigmoid`  activation function.\n",
        "\n",
        "Biases are initialized to zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swHwLXOI9E7V"
      },
      "source": [
        "# LSTM \n",
        "You are making this network, please note f, i, c and o (also \"v\") in the image below:\n",
        "![alt text](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
        "\n",
        "Please note that we are concatenating the old_hidden_vector and new_input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0DBzNY-90s5"
      },
      "source": [
        "# Quiz Question 5\n",
        "\n",
        "In the class definition below, what should be size_a, size_b, and size_c? ONLY use the variables defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFuHhqVq6Wge"
      },
      "source": [
        "size_a = Hidden_Layer_size # write your code here\n",
        "size_b = z_size # write your code here\n",
        "size_c = X_size # write your code here\n",
        "\n",
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.W_f = Param('W_f', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_f = Param('b_f', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_i = Param('W_i', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_i = Param('b_i', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_C = Param('W_C', np.random.randn(size_a, size_b) * weight_sd)\n",
        "        self.b_C = Param('b_C', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_o = Param('W_o', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_o = Param('b_o', np.zeros((size_a, 1)))\n",
        "\n",
        "        #For final layer to predict the next character\n",
        "        self.W_v = Param('W_v', np.random.randn(X_size, size_a) * weight_sd)\n",
        "        self.b_v = Param('b_v', np.zeros((size_c, 1)))\n",
        "        \n",
        "    def all(self):\n",
        "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
        "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
        "        \n",
        "parameters = Parameters()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzmfGLZt_xVs"
      },
      "source": [
        "Look at these operations which we'll be writing:\n",
        "\n",
        "**Concatenation of h and x:**\n",
        "\n",
        "$z\\:=\\:\\left[h_{t-1},\\:x\\right]$\n",
        "\n",
        "$f_t=\\sigma\\left(W_f\\cdot z\\:+\\:b_f\\:\\right)$\n",
        "\n",
        "$i_i=\\sigma\\left(W_i\\cdot z\\:+\\:b_i\\right)$\n",
        "\n",
        "$\\overline{C_t}=\\tanh\\left(W_C\\cdot z\\:+\\:b_C\\right)$\n",
        "\n",
        "$C_t=f_t\\ast C_{t-1}+i_t\\ast \\overline{C}_t$\n",
        "\n",
        "$o_t=\\sigma\\left(W_o\\cdot z\\:+\\:b_o\\right)$\n",
        "\n",
        "$h_t=o_t\\ast\\tanh\\left(C_t\\right)$\n",
        "\n",
        "**Logits:**\n",
        "\n",
        "$v_t=W_v\\cdot h_t+b_v$\n",
        "\n",
        "**Softmax:**\n",
        "\n",
        "$\\hat{y}=softmax\\left(v_t\\right)$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bUkseNnDott"
      },
      "source": [
        "def forward(x, h_prev, C_prev, p = parameters):\n",
        "    assert x.shape == (X_size, 1)\n",
        "    assert h_prev.shape == (Hidden_Layer_size, 1)\n",
        "    assert C_prev.shape == (Hidden_Layer_size, 1)\n",
        "    \n",
        "    z = np.row_stack((h_prev, x))\n",
        "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
        "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
        "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
        "\n",
        "    C = f * C_prev + i * C_bar\n",
        "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
        "    h = o * tanh(C)\n",
        "\n",
        "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
        "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
        "\n",
        "    return z, f, i, C_bar, C, o, h, v, y"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZrDhZIjFpdI"
      },
      "source": [
        "You must finish the function above before you can attempt the questions below. \n",
        "\n",
        "# Quiz Question 6\n",
        "\n",
        "What is the output of 'print(len(forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)), parameters)))'?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmKU4RP8M53y",
        "outputId": "9df0c9b6-35eb-4e6e-8698-885afba22c35"
      },
      "source": [
        "print(len(forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)), parameters)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV-YVl_GGiX8"
      },
      "source": [
        "# Quiz Question 7 \n",
        "\n",
        "Assuming you have fixed the forward function, run this command: \n",
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))\n",
        "\n",
        "Now, find these values:\n",
        "\n",
        "\n",
        "1.   print(z.shape)\n",
        "2.   print(np.sum(z))\n",
        "3.   print(np.sum(f))\n",
        "\n",
        "Copy and paste exact values you get in the logs into the quiz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GvKVWmTDt3H"
      },
      "source": [
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8pdZv9_NBPl",
        "outputId": "938817ac-ec70-4239-b21b-e70ffe43b1a8"
      },
      "source": [
        "print(z.shape)\n",
        "print(np.sum(z))\n",
        "print(np.sum(f))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(85, 1)\n",
            "0.0\n",
            "5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeSvhkqwILsG"
      },
      "source": [
        "# Backpropagation\n",
        "\n",
        "Here we are defining the backpropagation. It's too complicated, here is the whole code. (Please note that this would work only if your earlier code is perfect)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIa1jUZiGPmF"
      },
      "source": [
        "def backward(target, dh_next, dC_next, C_prev,\n",
        "             z, f, i, C_bar, C, o, h, v, y,\n",
        "             p = parameters):\n",
        "    \n",
        "    assert z.shape == (X_size + Hidden_Layer_size, 1)\n",
        "    assert v.shape == (X_size, 1)\n",
        "    assert y.shape == (X_size, 1)\n",
        "    \n",
        "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
        "        assert param.shape == (Hidden_Layer_size, 1)\n",
        "        \n",
        "    dv = np.copy(y)\n",
        "    dv[target] -= 1\n",
        "\n",
        "    p.W_v.d += np.dot(dv, h.T)\n",
        "    p.b_v.d += dv\n",
        "\n",
        "    dh = np.dot(p.W_v.v.T, dv)        \n",
        "    dh += dh_next\n",
        "    do = dh * tanh(C)\n",
        "    do = dsigmoid(o) * do\n",
        "    p.W_o.d += np.dot(do, z.T)\n",
        "    p.b_o.d += do\n",
        "\n",
        "    dC = np.copy(dC_next)\n",
        "    dC += dh * o * dtanh(tanh(C))\n",
        "    dC_bar = dC * i\n",
        "    dC_bar = dtanh(C_bar) * dC_bar\n",
        "    p.W_C.d += np.dot(dC_bar, z.T)\n",
        "    p.b_C.d += dC_bar\n",
        "\n",
        "    di = dC * C_bar\n",
        "    di = dsigmoid(i) * di\n",
        "    p.W_i.d += np.dot(di, z.T)\n",
        "    p.b_i.d += di\n",
        "\n",
        "    df = dC * C_prev\n",
        "    df = dsigmoid(f) * df\n",
        "    p.W_f.d += np.dot(df, z.T)\n",
        "    p.b_f.d += df\n",
        "\n",
        "    dz = (np.dot(p.W_f.v.T, df)\n",
        "         + np.dot(p.W_i.v.T, di)\n",
        "         + np.dot(p.W_C.v.T, dC_bar)\n",
        "         + np.dot(p.W_o.v.T, do))\n",
        "    dh_prev = dz[:Hidden_Layer_size, :]\n",
        "    dC_prev = f * dC\n",
        "    \n",
        "    return dh_prev, dC_prev"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnc7WpRkIU5S"
      },
      "source": [
        "# Forward and Backward Combined Pass\n",
        "\n",
        "Let's first clear the gradients before each backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJWoC3U1ITf8"
      },
      "source": [
        "def clear_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.d.fill(0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XN93UnjIgmA"
      },
      "source": [
        "Clip gradients to mitigate exploding gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LTsublxIfFl"
      },
      "source": [
        "def clip_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        np.clip(p.d, -1, 1, out=p.d)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7XUpDTWIl_Y"
      },
      "source": [
        "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
        "\n",
        "input, target are list of integers, with character indexes.\n",
        "h_prev is the array of initial h at  h−1  (size H x 1)\n",
        "C_prev is the array of initial C at  C−1  (size H x 1)\n",
        "Returns loss, final  hT  and  CT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQNxjTuZIia_"
      },
      "source": [
        "def forward_backward(inputs, targets, h_prev, C_prev):\n",
        "    global paramters\n",
        "    \n",
        "    # To store the values for each time step\n",
        "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
        "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
        "    v_s, y_s =  {}, {}\n",
        "    \n",
        "    # Values at t - 1\n",
        "    h_s[-1] = np.copy(h_prev)\n",
        "    C_s[-1] = np.copy(C_prev)\n",
        "    \n",
        "    loss = 0\n",
        "    # Loop through time steps\n",
        "    assert len(inputs) == Time_steps\n",
        "    for t in range(len(inputs)):\n",
        "        x_s[t] = np.zeros((X_size, 1))\n",
        "        x_s[t][inputs[t]] = 1 # Input character\n",
        "        \n",
        "        (z_s[t], f_s[t], i_s[t],\n",
        "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
        "        v_s[t], y_s[t]) = \\\n",
        "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
        "            \n",
        "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
        "        \n",
        "    clear_gradients()\n",
        "\n",
        "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
        "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        # Backward pass\n",
        "        dh_next, dC_next = \\\n",
        "            backward(target = targets[t], dh_next = dh_next,\n",
        "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
        "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
        "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
        "                     y = y_s[t])\n",
        "\n",
        "    clip_gradients()\n",
        "        \n",
        "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcy5u_vRItkV"
      },
      "source": [
        "# Sample the next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8SrtJiwIsSm"
      },
      "source": [
        "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
        "    x = np.zeros((X_size, 1))\n",
        "    x[first_char_idx] = 1\n",
        "\n",
        "    h = h_prev\n",
        "    C = C_prev\n",
        "\n",
        "    indexes = []\n",
        "    \n",
        "    for t in range(sentence_length):\n",
        "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
        "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
        "        x = np.zeros((X_size, 1))\n",
        "        x[idx] = 1\n",
        "        indexes.append(idx)\n",
        "\n",
        "    return indexes"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiWFaWLNIx_L"
      },
      "source": [
        "# Training (Adagrad)\n",
        "\n",
        "Update the graph and display a sample output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENQYU-7AIw0t"
      },
      "source": [
        "def update_status(inputs, h_prev, C_prev):\n",
        "    #initialized later\n",
        "    global plot_iter, plot_loss\n",
        "    global smooth_loss\n",
        "    \n",
        "    # Get predictions for 200 letters with current model\n",
        "\n",
        "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
        "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
        "\n",
        "    # Clear and plot\n",
        "    plt.plot(plot_iter, plot_loss)\n",
        "    display.clear_output(wait=True)\n",
        "    plt.show()\n",
        "\n",
        "    #Print prediction and loss\n",
        "    print(\"----\\n %s \\n----\" % (txt, ))\n",
        "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACXcASJuI73a"
      },
      "source": [
        "# Update Parameters\n",
        "\n",
        "\\begin{align}\n",
        "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
        "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR08TvcjI4Pf"
      },
      "source": [
        "def update_paramters(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.m += p.d * p.d # Calculate sum of gradients\n",
        "        #print(learning_rate * dparam)\n",
        "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La9vyJ6RJLFK"
      },
      "source": [
        "To delay the keyboard interrupt to prevent the training from stopping in the middle of an iteration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVDHbMb7JNGT"
      },
      "source": [
        "# Exponential average of loss\n",
        "# Initialize to a error of a random model\n",
        "smooth_loss = -np.log(1.0 / X_size) * Time_steps\n",
        "\n",
        "iteration, pointer = 0, 0\n",
        "\n",
        "# For the graph\n",
        "plot_iter = np.zeros((0))\n",
        "plot_loss = np.zeros((0))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF6vS0VWJqsS"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edhClIWVO0km"
      },
      "source": [
        "## All the previous cells"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8owA9Ba0O_Z7"
      },
      "source": [
        "Hidden_Layer_size = 100 #size of the hidden layer\n",
        "Time_steps = 40 # Number of time steps (length of the sequence) used for training\n",
        "learning_rate = 1e-1 # Learning Rate\n",
        "weight_sd = 0.1 #Standard deviation of weights for initialization\n",
        "z_size = Hidden_Layer_size + X_size #Size of concatenation(H, X) vector"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdGBImuJO_aE"
      },
      "source": [
        "class Param:\n",
        "    def __init__(self, name, value):\n",
        "      self.name = name\n",
        "      self.v = value # parameter value\n",
        "      self.d = np.zeros_like(value) # derivative\n",
        "      self.m = np.zeros_like(value) # momentum for Adagrad"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS3jVfjjO_aF"
      },
      "source": [
        "size_a = Hidden_Layer_size # write your code here\n",
        "size_b = z_size # write your code here\n",
        "size_c = X_size # write your code here\n",
        "\n",
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.W_f = Param('W_f', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_f = Param('b_f', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_i = Param('W_i', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_i = Param('b_i', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_C = Param('W_C', np.random.randn(size_a, size_b) * weight_sd)\n",
        "        self.b_C = Param('b_C', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_o = Param('W_o', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_o = Param('b_o', np.zeros((size_a, 1)))\n",
        "\n",
        "        #For final layer to predict the next character\n",
        "        self.W_v = Param('W_v', np.random.randn(X_size, size_a) * weight_sd)\n",
        "        self.b_v = Param('b_v', np.zeros((size_c, 1)))\n",
        "        \n",
        "    def all(self):\n",
        "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
        "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
        "        \n",
        "parameters = Parameters()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl0p3Hi0O_aG"
      },
      "source": [
        "def forward(x, h_prev, C_prev, p = parameters):\n",
        "    assert x.shape == (X_size, 1)\n",
        "    assert h_prev.shape == (Hidden_Layer_size, 1)\n",
        "    assert C_prev.shape == (Hidden_Layer_size, 1)\n",
        "    \n",
        "    z = np.row_stack((h_prev, x))\n",
        "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
        "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
        "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
        "\n",
        "    C = f * C_prev + i * C_bar\n",
        "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
        "    h = o * tanh(C)\n",
        "\n",
        "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
        "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
        "\n",
        "    return z, f, i, C_bar, C, o, h, v, y"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAqCVuAOO_aH"
      },
      "source": [
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zysgmuXaO_aH"
      },
      "source": [
        "def backward(target, dh_next, dC_next, C_prev,\n",
        "             z, f, i, C_bar, C, o, h, v, y,\n",
        "             p = parameters):\n",
        "    \n",
        "    assert z.shape == (X_size + Hidden_Layer_size, 1)\n",
        "    assert v.shape == (X_size, 1)\n",
        "    assert y.shape == (X_size, 1)\n",
        "    \n",
        "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
        "        assert param.shape == (Hidden_Layer_size, 1)\n",
        "        \n",
        "    dv = np.copy(y)\n",
        "    dv[target] -= 1\n",
        "\n",
        "    p.W_v.d += np.dot(dv, h.T)\n",
        "    p.b_v.d += dv\n",
        "\n",
        "    dh = np.dot(p.W_v.v.T, dv)        \n",
        "    dh += dh_next\n",
        "    do = dh * tanh(C)\n",
        "    do = dsigmoid(o) * do\n",
        "    p.W_o.d += np.dot(do, z.T)\n",
        "    p.b_o.d += do\n",
        "\n",
        "    dC = np.copy(dC_next)\n",
        "    dC += dh * o * dtanh(tanh(C))\n",
        "    dC_bar = dC * i\n",
        "    dC_bar = dtanh(C_bar) * dC_bar\n",
        "    p.W_C.d += np.dot(dC_bar, z.T)\n",
        "    p.b_C.d += dC_bar\n",
        "\n",
        "    di = dC * C_bar\n",
        "    di = dsigmoid(i) * di\n",
        "    p.W_i.d += np.dot(di, z.T)\n",
        "    p.b_i.d += di\n",
        "\n",
        "    df = dC * C_prev\n",
        "    df = dsigmoid(f) * df\n",
        "    p.W_f.d += np.dot(df, z.T)\n",
        "    p.b_f.d += df\n",
        "\n",
        "    dz = (np.dot(p.W_f.v.T, df)\n",
        "         + np.dot(p.W_i.v.T, di)\n",
        "         + np.dot(p.W_C.v.T, dC_bar)\n",
        "         + np.dot(p.W_o.v.T, do))\n",
        "    dh_prev = dz[:Hidden_Layer_size, :]\n",
        "    dC_prev = f * dC\n",
        "    \n",
        "    return dh_prev, dC_prev"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeBM6dyVO_aI"
      },
      "source": [
        "def clear_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.d.fill(0)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRE1ycA0O_aI"
      },
      "source": [
        "def clip_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        np.clip(p.d, -1, 1, out=p.d)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsHpPe3gO_aJ"
      },
      "source": [
        "def forward_backward(inputs, targets, h_prev, C_prev):\n",
        "    global paramters\n",
        "    \n",
        "    # To store the values for each time step\n",
        "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
        "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
        "    v_s, y_s =  {}, {}\n",
        "    \n",
        "    # Values at t - 1\n",
        "    h_s[-1] = np.copy(h_prev)\n",
        "    C_s[-1] = np.copy(C_prev)\n",
        "    \n",
        "    loss = 0\n",
        "    # Loop through time steps\n",
        "    assert len(inputs) == Time_steps\n",
        "    for t in range(len(inputs)):\n",
        "        x_s[t] = np.zeros((X_size, 1))\n",
        "        x_s[t][inputs[t]] = 1 # Input character\n",
        "        \n",
        "        (z_s[t], f_s[t], i_s[t],\n",
        "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
        "        v_s[t], y_s[t]) = \\\n",
        "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
        "            \n",
        "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
        "        \n",
        "    clear_gradients()\n",
        "\n",
        "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
        "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        # Backward pass\n",
        "        dh_next, dC_next = \\\n",
        "            backward(target = targets[t], dh_next = dh_next,\n",
        "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
        "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
        "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
        "                     y = y_s[t])\n",
        "\n",
        "    clip_gradients()\n",
        "        \n",
        "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L368yJm7O_aK"
      },
      "source": [
        "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
        "    x = np.zeros((X_size, 1))\n",
        "    x[first_char_idx] = 1\n",
        "\n",
        "    h = h_prev\n",
        "    C = C_prev\n",
        "\n",
        "    indexes = []\n",
        "    \n",
        "    for t in range(sentence_length):\n",
        "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
        "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
        "        x = np.zeros((X_size, 1))\n",
        "        x[idx] = 1\n",
        "        indexes.append(idx)\n",
        "\n",
        "    return indexes"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUPQpEimO_aK"
      },
      "source": [
        "def update_status(inputs, h_prev, C_prev):\n",
        "    #initialized later\n",
        "    global plot_iter, plot_loss\n",
        "    global smooth_loss\n",
        "    \n",
        "    # Get predictions for 200 letters with current model\n",
        "\n",
        "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
        "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
        "\n",
        "    # Clear and plot\n",
        "    plt.plot(plot_iter, plot_loss)\n",
        "    display.clear_output(wait=True)\n",
        "    plt.show()\n",
        "\n",
        "    #Print prediction and loss\n",
        "    print(\"----\\n %s \\n----\" % (txt, ))\n",
        "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ho1YKN0O_aL"
      },
      "source": [
        "def update_paramters(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.m += p.d * p.d # Calculate sum of gradients\n",
        "        #print(learning_rate * dparam)\n",
        "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVHGY4wUOzHr"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH7ikVbNO3Gg"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQyNSL0iJOxH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "1fb5ce32-1331-4ff6-b0c0-710382c849ac"
      },
      "source": [
        "iter = 50_000\n",
        "while iter > 0:\n",
        "  # Reset\n",
        "  if pointer + Time_steps >= len(data) or iteration == 0:\n",
        "      g_h_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      g_C_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      pointer = 0\n",
        "\n",
        "\n",
        "  inputs = ([char_to_idx[ch] \n",
        "              for ch in data[pointer: pointer + Time_steps]])\n",
        "  targets = ([char_to_idx[ch] \n",
        "              for ch in data[pointer + 1: pointer + Time_steps + 1]])\n",
        "\n",
        "  loss, g_h_prev, g_C_prev = \\\n",
        "      forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "  # Print every hundred steps\n",
        "  if iteration % 100 == 0:\n",
        "      update_status(inputs, g_h_prev, g_C_prev)\n",
        "\n",
        "  update_paramters()\n",
        "\n",
        "  plot_iter = np.append(plot_iter, [iteration])\n",
        "  plot_loss = np.append(plot_loss, [loss])\n",
        "\n",
        "  pointer += Time_steps\n",
        "  iteration += 1\n",
        "  iter = iter -1"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " Easore for the virus of China in Wuhan thavel.\n",
            "\n",
            "That county. But is weres you treat cume \n",
            "Arulines and their life, most lical hospiralines aid kot by touching roseanfities, and the aispititas doakes t \n",
            "----\n",
            "iter 49900, loss 4.717118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AKpa1BGOItQ"
      },
      "source": [
        "# Quiz Question 8\n",
        "\n",
        "Run the above code for 50000 iterations making sure that you have 100 hidden layers and time_steps is 40. What is the loss value you're seeing?"
      ]
    }
  ]
}